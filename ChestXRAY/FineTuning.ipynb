{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code adapted by Edoardo Giacomello\n",
    "Original Author: Luca Nassano\n",
    "'''\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow_addons as tfa\n",
    "from skimage.transform import warp, AffineTransform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "tf.random.set_seed(42)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def apply_random_scaling(image, minval=-.02, maxval=.02):\n",
    "    param = tf.random.uniform([], minval=minval, maxval=maxval)\n",
    "    source_size = image.shape\n",
    "    target_size = tf.cast(source_size[0]*(1.0+param), tf.int32), tf.cast(source_size[1]*(1.0+param), tf.int32)\n",
    "    output = tf.image.resize_with_crop_or_pad(tf.image.resize(image, target_size), source_size[0], source_size[1])\n",
    "    return output, param\n",
    "\n",
    "def apply_random_shearing(image, minval=-5., maxval=5.):\n",
    "    #param = tf.random.uniform([], minval=tf.math.atan(minval/image.shape[1]), maxval=tf.math.atan(maxval/image.shape[1]))\n",
    "    #param = tf.random.uniform([], minval=tf.math.atan(), maxval=tf.math.atan(maxval/image.shape[1]))\n",
    "    param = np.random.uniform(low=minval, high=maxval)\n",
    "    output = warp(np.array(image), AffineTransform(shear=np.arctan(param/image.shape[1])).inverse)\n",
    "    return output, param\n",
    "@tf.function\n",
    "def apply_random_rotation(image, minval=-7, maxval=7):\n",
    "    param = tf.random.uniform([], minval=minval, maxval=maxval)\n",
    "    output = tfa.image.rotate(image, param*math.pi/180.0, interpolation='BILINEAR')\n",
    "    return output, param\n",
    "\n",
    "\n",
    "def apply_test_time_augmentation(image, labels, image_id):\n",
    "    '''Implements TTA, https://arxiv.org/pdf/1911.06475.pdf pag13:\n",
    "    \n",
    "    (...) for each test CXR, we applied a random \n",
    "    transformation (amongst horizontal flipping, \n",
    "    rotating ±7 degrees, scaling±2%,and shearing±5 pixels) 10 times (...)\n",
    "    \n",
    "    :param image - the input image\n",
    "    :param labels - the labels associated with the image\n",
    "    :param image_id - an ordinal or id associated with the image\n",
    "    \n",
    "    :returns - a DataFrame containing one row for each generated image (+1 for the original one), a list of generated images and labels. \n",
    "    The dataframe contains the augmentation method used, the parameter and the image/label filenames.\n",
    "    '''\n",
    "    dataframe = pd.DataFrame()\n",
    "    image_list = list()\n",
    "    image_list.append((image, labels))\n",
    "    \n",
    "    dataframe = dataframe.append({'image_id':image_id,\n",
    "                                      'tta_id':0,\n",
    "                                      'image_fn':'{}_{}_image.npy'.format(image_id, 0),\n",
    "                                      'labels_fn':'{}_label.npy'.format(image_id),\n",
    "                                      'method':'ORIGINAL',\n",
    "                                      'param':0.0}, ignore_index=True)\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "\n",
    "        random_function = tf.random.uniform([], minval=0, maxval=4, dtype=tf.int32)\n",
    "\n",
    "        output = image\n",
    "        param = tf.constant(0.0)        \n",
    "        \n",
    "        if tf.equal(random_function, 0):\n",
    "            output = tf.image.flip_left_right(image)\n",
    "            param = tf.constant(0.0)\n",
    "            method = 'FLIP'\n",
    "        if tf.equal(random_function, 1):\n",
    "            output, param = apply_random_rotation(image)\n",
    "            method = 'ROTATION'\n",
    "        if tf.equal(random_function, 2):\n",
    "            output, param = apply_random_scaling(image)\n",
    "            method = 'SCALING'\n",
    "        if tf.equal(random_function, 3):\n",
    "            output, param = apply_random_shearing(image)\n",
    "            method = 'SHEAR'\n",
    "        image_list.append((output, labels))\n",
    "        \n",
    "        dataframe = dataframe.append({'image_id':image_id,\n",
    "                                      'tta_id':int(i),\n",
    "                                      'image_fn':'{}_{}_image.npy'.format(image_id, i),\n",
    "                                      'labels_fn':'{}_label.npy'.format(image_id),\n",
    "                                      'method':method,\n",
    "                                      'param':float(param)}, ignore_index=True)\n",
    "    return dataframe, image_list\n",
    "\n",
    "\n",
    "def record_parser(example, image_size=224):\n",
    "    example_fmt = {\n",
    "        'label': tf.io.FixedLenFeature([14], tf.float32),\n",
    "        'image': tf.io.FixedLenFeature([],tf.string, default_value='')}\n",
    "    parsed = tf.io.parse_single_example(example, example_fmt)\n",
    "    image = tf.io.decode_png(parsed[\"image\"],channels=3)\n",
    "    image.set_shape([image_size, image_size, 3])\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image, parsed['label']\n",
    "\n",
    "def normalize_image(img,labels):\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "    img = (img - imagenet_mean) / imagenet_std\n",
    "    return img,labels\n",
    "\n",
    "def make_dataset(filename, image_size=224):\n",
    "  base_path = 'datasets/'\n",
    "  full_path = os.path.join(base_path,filename)\n",
    "  dataset = tf.data.TFRecordDataset(full_path)\n",
    "  parser = lambda x, size=image_size: record_parser(x, image_size=size)\n",
    "  parsed_dataset = dataset.map(parser,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "  parsed_dataset = parsed_dataset.map(normalize_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "  return parsed_dataset\n",
    "\n",
    "batch_size = 64\n",
    "#train_dataset = make_dataset('training_cropped.tfrecords').shuffle(buffer_size=128).batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "#train_dataset = make_dataset('training_cropped.tfrecords').batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "# UNCOMMENT TO ENABLE TRAINING\n",
    "cond_train_dataset = make_dataset('conditional_training.tfrecords').shuffle(buffer_size=128).batch(batch_size).prefetch(1) # Dataset of only positive parents, used for pre-training\n",
    "train_dataset = make_dataset('training_cropped.tfrecords').shuffle(buffer_size=128).batch(batch_size).prefetch(1) # Full dataset, using for fine-tuning the network\n",
    "val_dataset = make_dataset('validation_cropped.tfrecords').shuffle(buffer_size=128).batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "# TODO: Since we cannot convert the full pipeline to tensorflow (due to shearing depending on Skimage), the make tta dataset will:\n",
    "# 1) Compute the dataset if not already present at the given path (using the classical for structure)\n",
    "# 2) Load the dataset from npy files as a tensorflow dataset\n",
    "\n",
    "# label_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "#                 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
    "# pd_train_labels = pd.DataFrame(columns=label_names)\n",
    "# pd_cond_train_labels =  pd.DataFrame(columns=label_names)\n",
    "\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Activation, Lambda\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINETUNING\n",
    "input_model = 'ModelsRetrained/DenseNet121/PreTrained/model-10.hdf5'\n",
    "\n",
    "model = tf.keras.models.load_model(input_model)\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy','AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder_name='FineTuned'\n",
    "outputFolder = 'ModelsRetrained/{}/{}'.format('DenseNet121', subfolder_name)\n",
    "if not os.path.exists(outputFolder):\n",
    "    os.makedirs(outputFolder)\n",
    "filepath=outputFolder+\"/model-{epoch:02d}.hdf5\"\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(step_decay)\n",
    "early_stopping = EarlyStopping(monitor='val_AUC',mode='max',patience=4)\n",
    "checkpoint_cb = ModelCheckpoint(filepath,save_best_only = False,save_weights_only = False,\n",
    "                               save_freq='epoch',verbose=False)\n",
    "\n",
    "\n",
    "tf_log= 'ModelsRetrained/{}/{}/TensorBoard/'.format('DenseNet121', subfolder_name)\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999)\n",
    "callbacks = list()\n",
    "callbacks.append(lr_scheduler)\n",
    "callbacks.append(checkpoint_cb)\n",
    "\n",
    "history_cond = model.fit(train_dataset,\n",
    "          epochs=50,\n",
    "          validation_data=val_dataset,\n",
    "          callbacks=callbacks,\n",
    "          verbose=1,initial_epoch = 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
