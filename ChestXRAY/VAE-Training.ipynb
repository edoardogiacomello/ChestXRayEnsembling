{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fallen-length",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version :  2.4.0-rc0\n",
      "NumPy version :  1.21.0\n",
      "OpenCV version 4.5.1\n",
      "gpu list :  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import cv2 as cv\n",
    "import funcs as ff\n",
    "import os\n",
    "from VAE import VAE\n",
    "from datetime import datetime\n",
    "\n",
    "# check that the environment is fine\n",
    "print('TensorFlow version : ', tf.__version__ )\n",
    "print('NumPy version : ', np.__version__ )\n",
    "print('OpenCV version', cv.__version__)\n",
    "\n",
    "print('gpu list : ', tf.config.list_physical_devices())\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd :  /Volumes/Seagate Backup Plus Drive/EXPERIMENTS\n",
      "serie dir :  /Volumes/Seagate Backup Plus Drive/EXPERIMENTS/Embeddings\n",
      "exp_dir :  /Volumes/Seagate Backup Plus Drive/EXPERIMENTS/Embeddings/TO-BE-DELETED\n",
      "logs dir :  /Volumes/Seagate Backup Plus Drive/EXPERIMENTS/Embeddings/TO-BE-DELETED/logs/fit/20210720-101904\n",
      "logs dir :  /Volumes/Seagate Backup Plus Drive/EXPERIMENTS/Embeddings/TO-BE-DELETED/logs/fit/20210720-101906\n",
      "last ckpt dir :  /Volumes/Seagate Backup Plus Drive/EXPERIMENTS/Embeddings/TO-BE-DELETED/last_ckpt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DIRECTORY SETTINGS\n",
    "'''\n",
    "cwd = '/Volumes/Seagate Backup Plus Drive/EXPERIMENTS'\n",
    "\n",
    "serie_dir = os.path.join(cwd, 'Embeddings')\n",
    "if not os.path.isdir(serie_dir):\n",
    "    os.makedirs(serie_dir)\n",
    "\n",
    "exp_dir = os.path.join(serie_dir, 'TO-BE-DELETED')\n",
    "if not os.path.isdir(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "\n",
    "log_dir = os.path.join(exp_dir, \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "time.sleep(2)\n",
    "log_dir2 = os.path.join(exp_dir, \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if not os.path.isdir(log_dir2):\n",
    "    os.makedirs(log_dir2)\n",
    "\n",
    "last_ckpt = os.path.join(exp_dir, 'last_ckpt')\n",
    "if not os.path.isdir(last_ckpt):\n",
    "  os.mkdir(last_ckpt)\n",
    "\n",
    "print('cwd : ', cwd)\n",
    "print('serie dir : ', serie_dir)\n",
    "print('exp_dir : ', exp_dir)\n",
    "print('logs dir : ', log_dir)\n",
    "print('logs dir : ', log_dir2)\n",
    "print('last ckpt dir : ', last_ckpt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function record_parser at 0x167d13b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function record_parser at 0x167d13b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def record_parser(example):\n",
    "    example_fmt = {\n",
    "        'label': tf.io.FixedLenFeature([14], tf.float32),\n",
    "        'image': tf.io.FixedLenFeature([],tf.string, default_value='')}\n",
    "    parsed = tf.io.parse_single_example(example, example_fmt)\n",
    "    image = tf.io.decode_png(parsed[\"image\"],channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image, parsed['label']\n",
    "\n",
    "def normalize_image(img,labels):\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "    img = (img - imagenet_mean) / imagenet_std\n",
    "    return img,labels\n",
    "\n",
    "def make_dataset(filename):\n",
    "    base_path = '/Volumes/Seagate Backup Plus Drive/datasets/chexpert-tfrecords'\n",
    "    full_path = os.path.join(base_path,filename)\n",
    "    dataset = tf.data.TFRecordDataset(full_path)\n",
    "    parsed_dataset = dataset.map(record_parser,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    #parsed_dataset = parsed_dataset.map(normalize_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    return parsed_dataset\n",
    "\n",
    "batch_size = 5\n",
    "train_ds = make_dataset('new_training_cropped.tfrecords').shuffle(buffer_size=batch_size*7).batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "val_ds = make_dataset('validation_cropped.tfrecords').shuffle(buffer_size=100).batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "test_ds = make_dataset('test_set_cropped.tfrecords').shuffle(buffer_size=50).batch(batch_size, drop_remainder=False).prefetch(1)\n",
    "\n",
    "label_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "                'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patent-fountain",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ###############\n",
    "# Model\n",
    "# ###############\n",
    "latent_dim = 200\n",
    "vae = VAE(number_of_classes=0,\n",
    "          latent_dim=latent_dim,\n",
    "          input_shape=(224, 224, 3),\n",
    "          backbone=tf.keras.applications.DenseNet121(include_top=False, pooling='avg'))\n",
    "vae.encoder.load_weights('/Volumes/Seagate Backup Plus Drive/models/ChestXray/ConditionalTraining/DenseNet121/FineTuning/model-05.hdf5', by_name=True)\n",
    "#vae.summary([224, 224, 3])\n",
    "#tf.keras.utils.plot_model(\n",
    "#    vae.vae, to_file=os.path.join(exp_dir, 'DenseNet121.png'), show_shapes=True, show_dtype=True,\n",
    "#    show_layer_names=True, rankdir='TB', expand_nested=True, dpi=96\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modular-chambers",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-20 10:19:07.967127: I tensorflow/python/profiler/internal/profiler_wrapper.cc:182] Profiling will start immediately because delay_ms was unset or zero.\n",
      "2021-07-20 10:19:07.967141: I tensorflow/core/profiler/lib/profiler_session.cc:133] Profiler session started.\n",
      "2021-07-20 10:19:07.972034: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-07-20 10:19:07.972138: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# ###############\n",
    "# TRAINING LOOP FUNCTIONS\n",
    "# ###############\n",
    "'''\n",
    "lr = 7.5e-4\n",
    "kl_weight = 5e-4\n",
    "epochs = 10\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "writer = tf.summary.create_file_writer(logdir=log_dir)\n",
    "loss = tf.keras.metrics.Mean(name='Training-Loss', dtype=tf.float32)\n",
    "latent_loss = tf.keras.metrics.Mean(name='Training-LatentLoss', dtype=tf.float32)\n",
    "rec_loss = tf.keras.metrics.Mean(name='Training-RecontructionLoss', dtype=tf.float32)\n",
    "val_loss = tf.keras.metrics.Mean(name='Validation-Loss', dtype=tf.float32)\n",
    "val_rec_loss = tf.keras.metrics.Mean(name='Validation-RecontructionLoss', dtype=tf.float32)\n",
    "val_latent_loss = tf.keras.metrics.Mean(name='Validation-LatentLoss', dtype=tf.float32)\n",
    "\n",
    "# TRAINING STEP\n",
    "@tf.function\n",
    "def vae_train_step(x, optimizer, kl_weight = 5e-4):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    # Feed input x into dbvae. Note that this is using the DB_VAE call function!\n",
    "    x_rec, z_mean, z_logsigma = vae(x)\n",
    "    loss, rec_loss, latent_loss = VAE.loss(x, x_rec, z_mean, z_logsigma, kl_weight)\n",
    "    # classification_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=y), axis=-1)\n",
    "  grads = tape.gradient(loss, vae.trainable_variables)\n",
    "  # apply gradients to variables\n",
    "  optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
    "  return loss, rec_loss, latent_loss\n",
    "\n",
    "# CALLBACKS\n",
    "class ImgPlotCB(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom callback to sistematically plot images\"\"\"\n",
    "    def __init__(self,\n",
    "                 samples = tf.zeros((128, 128)),\n",
    "                 model = None,\n",
    "                 plot_freq = 200\n",
    "                 ):\n",
    "        super(ImgPlotCB, self).__init__()\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        self.orig_imgs = samples\n",
    "        self.orig_normlzd = ff.normalize_tensor(samples)\n",
    "        self.plot_freq = plot_freq\n",
    "        self.x_rec = None\n",
    "        self.z_mean = None\n",
    "        self.z_logsigma = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if not bool(batch % self.plot_freq):\n",
    "            self._plot_imgs()\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        display.clear_output(wait=False)\n",
    "        # sanity check for mu and sigma\n",
    "        print('z_mean mean : {}'.format(tf.math.reduce_mean(self.z_mean)))\n",
    "        print('z_logsigma mean : {}'.format(tf.math.reduce_mean(self.z_logsigma)))\n",
    "        stddev = tf.math.exp(self.z_logsigma)\n",
    "\n",
    "        # sample three times to sanity-check the decoding\n",
    "        for i in range(3):\n",
    "            z_new = tf.random.normal(shape= tf.shape(self.z_mean), mean=self.z_mean, stddev=stddev)\n",
    "            new_rec = self.model.decode(z_new)\n",
    "            new_rec = ff.normalize_tensor(new_rec)\n",
    "            fig = plt.figure(i+3)\n",
    "            plt.title('New Sampling {}'.format(i), color='red')\n",
    "            for i in range(new_rec.shape[0]):\n",
    "                plt.subplot(1, 4, i + 1)\n",
    "                plt.imshow(new_rec[i, :, :, :])\n",
    "                plt.axis('off')\n",
    "        self._plot_imgs()\n",
    "\n",
    "        return\n",
    "\n",
    "    def _plot_imgs(self):\n",
    "        self.x_rec, self.z_mean, self.z_logsigma = self.model(self.orig_imgs)\n",
    "        img = ff.normalize_tensor(self.x_rec)\n",
    "\n",
    "        fig = plt.figure(1)\n",
    "        plt.title('Images', color='red')\n",
    "        for i in range(img.shape[0]):\n",
    "            plt.subplot(1, 4, i + 1)\n",
    "            plt.imshow(img[i, :, :, :])\n",
    "            plt.axis('off')\n",
    "\n",
    "        fig = plt.figure(2)\n",
    "        plt.title('Original Images', color='green')\n",
    "        for i in range(self.orig_normlzd.shape[0]):\n",
    "            plt.subplot(1, 4, i + 1)\n",
    "            plt.imshow(self.orig_normlzd[i, :, :, :])\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # ###################\n",
    "    # CUSTOM CALLBACK END\n",
    "    # ###################\n",
    "\n",
    "class KLWeightScheduler(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom callback to increase the kl regularization effect, making it sleep\n",
    "    for a few epochs at the start\"\"\"\n",
    "    def __init__(self,\n",
    "                 initial_weight = 1e-3,\n",
    "                 patience = 10,\n",
    "                 max_value = 0.5,\n",
    "                 factor = 1.1\n",
    "                 ):\n",
    "        super(KLWeightScheduler, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.KL_weight = 0.\n",
    "        self.max_val = max_value\n",
    "        self.factor = factor\n",
    "        self.initial_weight = initial_weight\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == (self.patience):\n",
    "            self.KL_weight = self.initial_weight\n",
    "        elif epoch > (self.patience):\n",
    "            self.KL_weight = self.initial_weight * (self.factor**epoch)\n",
    "            self.KL_weight = tf.math.minimum(self.KL_weight, self.max_val)\n",
    "        logs['KL_weight'] = self.KL_weight\n",
    "        print('Kullback-Leibler weight : {}'.format(self.KL_weight))\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        return\n",
    "\n",
    "    # ###################\n",
    "    # CUSTOM CALLBACK END\n",
    "    # ###################\n",
    "\n",
    "class ReduceLROnPlateau(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self,\n",
    "                  factor=0.1,\n",
    "                  patience=10,\n",
    "                  verbose=0,\n",
    "                  mode='auto',\n",
    "                  min_delta=1e-4,\n",
    "                  cooldown=0,\n",
    "                  min_lr=0,\n",
    "                  sign_number = 4,\n",
    "                  optim_lr = None,\n",
    "                  reduce_lin = False):\n",
    "        super(ReduceLROnPlateau, self).__init__()\n",
    "        # Optimizer Error Handling\n",
    "        if not tf.is_tensor(optim_lr):\n",
    "            raise ValueError('Need optimizer !')\n",
    "        if factor >= 1.0:\n",
    "            raise ValueError('ReduceLROnPlateau ' 'does not support a factor >= 1.0.')\n",
    "        # Passing optimizer as arguement\n",
    "        self.optim_lr = optim_lr\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.cooldown = cooldown\n",
    "        self.cooldown_counter = 0  # Cooldown counter.\n",
    "        self.wait = 0\n",
    "        self.best = 0\n",
    "        self.mode = mode\n",
    "        self.monitor_op = None\n",
    "        self.sign_number = sign_number\n",
    "\n",
    "\n",
    "        ## Custom modification: linearly reducing learning\n",
    "        self.reduce_lin = reduce_lin\n",
    "        self.reduce_lr = True\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets wait counter and cooldown counter.\n",
    "        \"\"\"\n",
    "        if self.mode not in ['auto', 'min', 'max']:\n",
    "            print('Learning Rate Plateau Reducing mode %s is unknown, '\n",
    "                            'fallback to auto mode.', self.mode)\n",
    "            self.mode = 'auto'\n",
    "        if self.mode == 'min' or (self.mode == 'auto'):\n",
    "            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n",
    "            self.best = np.Inf\n",
    "        else:\n",
    "            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n",
    "            self.best = -np.Inf\n",
    "        self.cooldown_counter = 0\n",
    "        self.wait = 0\n",
    "\n",
    "    def in_cooldown(self):\n",
    "        return self.cooldown_counter > 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, loss, logs=None):\n",
    "        logs = logs or {}\n",
    "        ## Custom modification: Optimizer\n",
    "        # logs['lr'] = K.get_value(self.model.optimizer.lr) returns a numpy array\n",
    "        # and therefore can be modified to\n",
    "        logs['lr'] = float(self.optim_lr.numpy())\n",
    "\n",
    "        ## Custom modification: Deprecated due to focusing on validation loss\n",
    "        # current = logs.get(self.monitor)\n",
    "\n",
    "        current = float(loss)\n",
    "\n",
    "        ## Custom modification: Deprecated due to focusing on validation loss\n",
    "        # if current is None:\n",
    "        #     print('Reduce LR on plateau conditioned on metric `%s` '\n",
    "        #                     'which is not available. Available metrics are: %s',\n",
    "        #                     self.monitor, ','.join(list(logs.keys())))\n",
    "\n",
    "        # else:\n",
    "\n",
    "        if self.in_cooldown():\n",
    "            self.cooldown_counter -= 1\n",
    "            self.wait = 0\n",
    "\n",
    "        if self.monitor_op(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "        elif not self.in_cooldown():\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "\n",
    "                ## Custom modification: Optimizer Learning Rate\n",
    "                # old_lr = float(K.get_value(self.model.optimizer.lr))\n",
    "                old_lr = float(self.optim_lr.numpy())\n",
    "                if old_lr > self.min_lr and self.reduce_lr == True:\n",
    "                    ## Custom modification: Linear learning Rate\n",
    "                    if self.reduce_lin == True:\n",
    "                        new_lr = old_lr - self.factor\n",
    "                        ## Custom modification: Error Handling when learning rate is below zero\n",
    "                        if new_lr <= 0:\n",
    "                            print('Learning Rate is below zero: {}, '\n",
    "                            'fallback to minimal learning rate: {}. '\n",
    "                            'Stop reducing learning rate during training.'.format(new_lr, self.min_lr))\n",
    "                            self.reduce_lr = False\n",
    "                    else:\n",
    "                        new_lr = old_lr * self.factor\n",
    "                    new_lr = max(new_lr, self.min_lr)\n",
    "                    print('Learning rate has been reduced.'\n",
    "                        'Old learning rate : {}'\n",
    "                        'New Learning rate : {}'.format(old_lr, new_lr))\n",
    "\n",
    "\n",
    "                    ## Custom modification: Optimizer Learning Rate\n",
    "                    # K.set_value(self.model.optimizer.lr, new_lr)\n",
    "                    self.optim_lr.assign(new_lr)\n",
    "\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: ReduceLROnPlateau reducing learning '\n",
    "                                'rate to %s.' % (epoch + 1, float(new_lr)))\n",
    "                    self.cooldown_counter = self.cooldown\n",
    "                    self.wait = 0\n",
    "\n",
    "\n",
    "class LRScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, learning_rate, schedule, max_val=None, min_val=None):\n",
    "        super(LRScheduler, self).__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.schedule = schedule\n",
    "        self.max = max_val\n",
    "        self.min = min_val\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        tmp = self.schedule(epoch, float(self.lr.numpy()))\n",
    "        tmp = tf.math.maximum(tmp, self.min) if self.min is not None else tmp\n",
    "        tmp = tf.math.minimum(tmp, self.max) if self.max is not None else tmp\n",
    "        self.lr.assign(tmp)\n",
    "        logs['lr'] = float(self.lr.numpy())\n",
    "        print('Current Learnig Rate : {}'.format(logs['lr']))\n",
    "\n",
    "class Logger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 logdir : str,\n",
    "                 logdir2 : str,\n",
    "                 scalars : list,\n",
    "                 names : list,\n",
    "                 batch_scalars : list = None,\n",
    "                 batch_scal_names : list = None,\n",
    "                 batch_log_freq = 300):\n",
    "        super(Logger, self).__init__()\n",
    "        self.scalars = scalars\n",
    "        self.names = names\n",
    "        self.batch_scal = batch_scalars\n",
    "        self.batch_scal_names = batch_scal_names\n",
    "        self.batch_log_freq = batch_log_freq\n",
    "        self.batch_writer = tf.summary.create_file_writer(logdir)\n",
    "        self.epoch_writer = tf.summary.create_file_writer(logdir2)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if not bool(batch%self.batch_log_freq):\n",
    "            print('Step : {}'.format(batch))\n",
    "            for val, name in zip(self.scalars, self.names):\n",
    "                print(name, val.result())\n",
    "                with self.batch_writer.as_default():\n",
    "                  tf.summary.scalar(data=val.result(), name=name, step=batch)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('keys : ', list(logs.keys()))\n",
    "        for val, name in zip(self.scalars, self.names):\n",
    "            if name == 'loss':\n",
    "                logs['loss'] = float(val.result().numpy())\n",
    "        '''with self.epoch_writer.as_default():\n",
    "            tf.summary.scalar(data=val.result(), name=name, step=epoch)\n",
    "    for key, value in logs:\n",
    "        with self.epoch_writer.as_default():\n",
    "            tf.summary.scalar(key, value, epoch)'''\n",
    "\n",
    "\n",
    "class SaveModels(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model : tf.keras.Model, name : str):\n",
    "        super(SaveModels, self).__init__()\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if not bool((batch+1)%300):\n",
    "            self.model.vae.save(os.path.join(last_ckpt, self.name+'-last'), save_format='h5')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.vae.save(os.path.join(exp_dir, self.name+'_epoch_{}'.format(epoch)), save_format='h5')\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 3:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*tf.math.exp(-0.2)\n",
    "lrscheduler_cb = LRScheduler(learning_rate=optimizer.learning_rate, schedule=lr_schedule)\n",
    "tb_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False, write_images=False, update_freq=100)\n",
    "tb_cb.set_model(vae)\n",
    "\n",
    "rnd = tf.random.uniform(shape=(), minval=3, maxval=batch_size, dtype=tf.dtypes.int32)\n",
    "tmp = train_ds.take(1)\n",
    "examples = []\n",
    "for i, ex in enumerate(tmp):\n",
    "    examples.append(ex[0][rnd, ...])\n",
    "    examples.append(ex[0][rnd-1, ...])\n",
    "    examples.append(ex[0][rnd-2, ...])\n",
    "    examples.append(ex[0][rnd-3, ...])\n",
    "examples = tf.convert_to_tensor(examples)\n",
    "imgplot_cb = ImgPlotCB(samples=examples, model=vae, plot_freq=300)\n",
    "progbar_cb = tf.keras.callbacks.ProgbarLogger('steps')\n",
    "klweight_cb = KLWeightScheduler(initial_weight=kl_weight, patience=3, factor=1.5)\n",
    "reducelronplateau_cb = ReduceLROnPlateau(factor=0.5, patience=0, reduce_lin=False, optim_lr=optimizer.learning_rate, cooldown=1, min_delta=100)\n",
    "savemodels_cb = SaveModels(vae, 'VAE{}-DenseNet121-'.format(latent_dim))\n",
    "log_cb = Logger(log_dir,\n",
    "                log_dir2,\n",
    "                scalars=[latent_loss, rec_loss, loss, val_latent_loss, val_rec_loss, val_loss],\n",
    "                names=['latent loss', 'reconstruction loss', 'loss', 'validation latent loss', 'validation reconstruction loss', 'validation loss'])\n",
    "                # batch_scalars=[latent_loss, rec_loss, loss],\n",
    "                # batch_scal_names=['latent loss-b', 'reconstruction loss-b', 'loss-b'])\n",
    "\n",
    "cb_list = [log_cb]\n",
    "callback_list = tf.keras.callbacks.CallbackList(callbacks=cb_list, model=vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 0\n",
      "latent loss tf.Tensor(108.159195, shape=(), dtype=float32)\n",
      "reconstruction loss tf.Tensor(76880.82, shape=(), dtype=float32)\n",
      "loss tf.Tensor(76880.82, shape=(), dtype=float32)\n",
      "validation latent loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation reconstruction loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "train step time : 3.586151123046875\n",
      "Learning Rate Check : 0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<01:00,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val step time : 3.1798110008239746\n",
      "keys :  []\n",
      "Step : 0\n",
      "latent loss tf.Tensor(104.12959, shape=(), dtype=float32)\n",
      "reconstruction loss tf.Tensor(68467.625, shape=(), dtype=float32)\n",
      "loss tf.Tensor(68467.625, shape=(), dtype=float32)\n",
      "validation latent loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation reconstruction loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "train step time : 1.2536101341247559\n",
      "Learning Rate Check : 0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:10<00:42,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val step time : 2.935929775238037\n",
      "keys :  []\n",
      "Step : 0\n",
      "latent loss tf.Tensor(134.36137, shape=(), dtype=float32)\n",
      "reconstruction loss tf.Tensor(51420.977, shape=(), dtype=float32)\n",
      "loss tf.Tensor(51420.977, shape=(), dtype=float32)\n",
      "validation latent loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation reconstruction loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "train step time : 1.069159984588623\n",
      "Learning Rate Check : 0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:14<00:32,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val step time : 2.8539209365844727\n",
      "keys :  []\n",
      "Step : 0\n",
      "latent loss tf.Tensor(164.93588, shape=(), dtype=float32)\n",
      "reconstruction loss tf.Tensor(29382.463, shape=(), dtype=float32)\n",
      "loss tf.Tensor(29382.463, shape=(), dtype=float32)\n",
      "validation latent loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation reconstruction loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "train step time : 1.0752639770507812\n",
      "Learning Rate Check : 0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:18<00:26,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val step time : 2.867645263671875\n",
      "keys :  []\n",
      "Step : 0\n",
      "latent loss tf.Tensor(192.21411, shape=(), dtype=float32)\n",
      "reconstruction loss tf.Tensor(32256.943, shape=(), dtype=float32)\n",
      "loss tf.Tensor(32256.943, shape=(), dtype=float32)\n",
      "validation latent loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation reconstruction loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "train step time : 1.0720372200012207\n",
      "Learning Rate Check : 0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:22<00:21,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val step time : 2.909493923187256\n",
      "keys :  []\n",
      "Step : 0\n",
      "latent loss tf.Tensor(204.18182, shape=(), dtype=float32)\n",
      "reconstruction loss tf.Tensor(39150.26, shape=(), dtype=float32)\n",
      "loss tf.Tensor(39150.26, shape=(), dtype=float32)\n",
      "validation latent loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation reconstruction loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "train step time : 1.0778300762176514\n",
      "Learning rate has been reduced.Old learning rate : 0.000750000006519258New Learning rate : 0.000375000003259629\n",
      "Learning Rate Check : 0.000375000003259629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:26<00:16,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val step time : 2.881290912628174\n",
      "keys :  []\n",
      "Step : 0\n",
      "latent loss tf.Tensor(216.54385, shape=(), dtype=float32)\n",
      "reconstruction loss tf.Tensor(37029.78, shape=(), dtype=float32)\n",
      "loss tf.Tensor(37029.78, shape=(), dtype=float32)\n",
      "validation latent loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation reconstruction loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "train step time : 1.1823432445526123\n",
      "Learning rate has been reduced.Old learning rate : 0.000375000003259629New Learning rate : 0.0001875000016298145\n",
      "Learning Rate Check : 0.0001875000016298145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:30<00:12,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val step time : 2.9714839458465576\n",
      "keys :  []\n",
      "Step : 0\n",
      "latent loss tf.Tensor(245.10005, shape=(), dtype=float32)\n",
      "reconstruction loss tf.Tensor(37231.594, shape=(), dtype=float32)\n",
      "loss tf.Tensor(37231.594, shape=(), dtype=float32)\n",
      "validation latent loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation reconstruction loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "train step time : 1.1099438667297363\n",
      "Learning Rate Check : 0.0001875000016298145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:34<00:08,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val step time : 2.849493980407715\n",
      "keys :  []\n",
      "Step : 0\n",
      "latent loss tf.Tensor(245.34053, shape=(), dtype=float32)\n",
      "reconstruction loss tf.Tensor(28545.25, shape=(), dtype=float32)\n",
      "loss tf.Tensor(28545.25, shape=(), dtype=float32)\n",
      "validation latent loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation reconstruction loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "train step time : 1.0854759216308594\n",
      "Learning Rate Check : 0.0001875000016298145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:38<00:04,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val step time : 2.837679147720337\n",
      "keys :  []\n",
      "Step : 0\n",
      "latent loss tf.Tensor(240.96245, shape=(), dtype=float32)\n",
      "reconstruction loss tf.Tensor(32914.14, shape=(), dtype=float32)\n",
      "loss tf.Tensor(32914.14, shape=(), dtype=float32)\n",
      "validation latent loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation reconstruction loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "validation loss tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "train step time : 1.1079869270324707\n",
      "Learning Rate Check : 0.0001875000016298145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:42<00:00,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val step time : 2.920198917388916\n",
      "keys :  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''TRAINING LOOP'''\n",
    "\n",
    "callback_list.on_train_begin()\n",
    "fake_ds = train_ds.take(3)\n",
    "reducelronplateau_cb.on_train_begin()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    callback_list.on_epoch_begin(epoch)\n",
    "    start_time = time.time()\n",
    "    for batch, (train_x, train_labels) in enumerate(fake_ds):\n",
    "        callback_list.on_batch_begin(batch)\n",
    "        l, rl, ll = vae_train_step(train_x, optimizer, klweight_cb.KL_weight)\n",
    "\n",
    "        #metrics\n",
    "        loss(l)\n",
    "        rec_loss(rl)\n",
    "        latent_loss(ll)\n",
    "\n",
    "        callback_list.on_batch_end(batch)\n",
    "    print('train step time : {}'.format(time.time() - start_time))\n",
    "    reducelronplateau_cb.on_epoch_end(epoch, float(loss.result().numpy()))\n",
    "    print('Learning Rate Check : {}'.format(float(optimizer.learning_rate.numpy())))\n",
    "    start_time = time.time()\n",
    "    for test_x, test_labels in val_ds.take(5):\n",
    "        x_rec, z_mean, z_logsigma = vae(test_x)\n",
    "        l, rl, ll = vae.loss(test_x, x_rec, z_mean, z_logsigma)\n",
    "\n",
    "        #metrics\n",
    "        val_loss(l)\n",
    "        val_rec_loss(rl)\n",
    "        val_latent_loss(ll)\n",
    "\n",
    "    print('val step time : {}'.format(time.time() - start_time))\n",
    "\n",
    "    callback_list.on_epoch_end(epoch)\n",
    "\n",
    "    # reset metrics\n",
    "    val_loss.reset_states()\n",
    "    val_rec_loss.reset_states()\n",
    "    val_latent_loss.reset_states()\n",
    "    loss.reset_states()\n",
    "    latent_loss.reset_states()\n",
    "    rec_loss.reset_states()\n",
    "\n",
    "callback_list.on_train_end()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}